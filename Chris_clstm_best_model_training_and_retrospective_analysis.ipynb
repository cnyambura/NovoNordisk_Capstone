{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First, load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "from keras.layers import Dropout\n",
    "from keras.layers.embeddings import Embedding\n",
    "\n",
    "import ndac\n",
    "import sklearn\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prest_id</th>\n",
       "      <th>uniprot_id</th>\n",
       "      <th>conc_cf</th>\n",
       "      <th>aa_seq</th>\n",
       "      <th>nt_seq</th>\n",
       "      <th>aa_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>140095</td>\n",
       "      <td>G3V3N0</td>\n",
       "      <td>4.3075</td>\n",
       "      <td>IMTAPSSFEQFKVAMNYLQLYNVPDCLEDIQDADCSSSKCSSSASS...</td>\n",
       "      <td>GACAAGCTTGCGGCCGCAATTATGACAGCTCCCTCCAGTTTTGAGC...</td>\n",
       "      <td>139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>140099</td>\n",
       "      <td>G3V537</td>\n",
       "      <td>2.9154</td>\n",
       "      <td>TYYAWKHELLGSGTCPALPPREVLGMEELEKLPEEQVAEEELECSA...</td>\n",
       "      <td>GACAAGCTTGCGGCCGCAACCTACTATGCCTGGAAGCATGAGCTGC...</td>\n",
       "      <td>144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>140225</td>\n",
       "      <td>P12724</td>\n",
       "      <td>1.4877</td>\n",
       "      <td>SLHARPPQFTRAQWFAIQHISLNPPRCTIAMRAINNYRWRCKNQNT...</td>\n",
       "      <td>GACAAGCTTGCGGCCGCATCACTCCATGCCAGACCCCCACAGTTTA...</td>\n",
       "      <td>136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>140235</td>\n",
       "      <td>H0YH02</td>\n",
       "      <td>6.7224</td>\n",
       "      <td>ARALNESKRVNNGNTAPEDSSPAKKTRRCQRQESKKMPVAGGKANK...</td>\n",
       "      <td>GACAAGCTTGCGGCCGCAGCGAGAGCATTAAATGAAAGCAAAAGAG...</td>\n",
       "      <td>123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>140309</td>\n",
       "      <td>F5GYC5</td>\n",
       "      <td>3.3848</td>\n",
       "      <td>HRKEPGARLEATRGAARPHKQGTKPMITRPSVSQLGEGKCPSSQHL...</td>\n",
       "      <td>GACAAGCTTGCGGCCGCACATCGGAAAGAGCCTGGGGCAAGGCTGG...</td>\n",
       "      <td>124</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   prest_id uniprot_id  conc_cf  \\\n",
       "0    140095     G3V3N0   4.3075   \n",
       "1    140099     G3V537   2.9154   \n",
       "2    140225     P12724   1.4877   \n",
       "3    140235     H0YH02   6.7224   \n",
       "4    140309     F5GYC5   3.3848   \n",
       "\n",
       "                                              aa_seq  \\\n",
       "0  IMTAPSSFEQFKVAMNYLQLYNVPDCLEDIQDADCSSSKCSSSASS...   \n",
       "1  TYYAWKHELLGSGTCPALPPREVLGMEELEKLPEEQVAEEELECSA...   \n",
       "2  SLHARPPQFTRAQWFAIQHISLNPPRCTIAMRAINNYRWRCKNQNT...   \n",
       "3  ARALNESKRVNNGNTAPEDSSPAKKTRRCQRQESKKMPVAGGKANK...   \n",
       "4  HRKEPGARLEATRGAARPHKQGTKPMITRPSVSQLGEGKCPSSQHL...   \n",
       "\n",
       "                                              nt_seq  aa_len  \n",
       "0  GACAAGCTTGCGGCCGCAATTATGACAGCTCCCTCCAGTTTTGAGC...     139  \n",
       "1  GACAAGCTTGCGGCCGCAACCTACTATGCCTGGAAGCATGAGCTGC...     144  \n",
       "2  GACAAGCTTGCGGCCGCATCACTCCATGCCAGACCCCCACAGTTTA...     136  \n",
       "3  GACAAGCTTGCGGCCGCAGCGAGAGCATTAAATGAAAGCAAAAGAG...     123  \n",
       "4  GACAAGCTTGCGGCCGCACATCGGAAAGAGCCTGGGGCAAGGCTGG...     124  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read original data from /gscratch/pfaendtner/cnyambura/NovoNordisk_Capstone/dataframes\n",
    "data = pd.read_csv('/gscratch/pfaendtner/cnyambura/NovoNordisk_Capstone/dataframes/DF_prest.csv', index_col=0)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#check shape of data\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup nt doc and classify expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data shape:  (22364, 8)\n"
     ]
    }
   ],
   "source": [
    "def nt_seq_doc(nt_sequence):\n",
    "    if 'GACAAGCTTGCGGCCGCA' not in nt_sequence:\n",
    "        return None\n",
    "    true_nt = nt_sequence.split('GACAAGCTTGCGGCCGCA')[1]\n",
    "    if len(true_nt) % 3 != 0:\n",
    "        return None\n",
    "    return ' '.join([true_nt[i:i+3] \n",
    "                     for i in range(0, len(true_nt), 3)])\n",
    "# split quantiles\n",
    "def assign_class(conc):\n",
    "    if conc <= low_cut:\n",
    "        return 0\n",
    "    elif conc >= high_cut:\n",
    "        return 1\n",
    "    return\n",
    "\n",
    "data['nt_seq_doc'] = data['nt_seq'].apply(nt_seq_doc)\n",
    "data = data[pd.notnull(data['nt_seq_doc'])]\n",
    "\n",
    "# identify high and low classes by conc_cf quantiles\n",
    "low_cut = data['conc_cf'].quantile(0.25)\n",
    "high_cut = data['conc_cf'].quantile(0.75)\n",
    "\n",
    "data['class'] = data['conc_cf'].apply(assign_class)\n",
    "data = data[pd.notnull(data['class'])]\n",
    "# check shape\n",
    "print('data shape: ', data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training and Data Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define sequence documents\n",
    "docs = list(data['nt_seq_doc'])\n",
    "# create the tokenizer\n",
    "t = Tokenizer()\n",
    "# fit the tokenizer on the documents\n",
    "t.fit_on_texts(docs)\n",
    "\n",
    "# integer encode documents\n",
    "X = t.texts_to_sequences(docs)\n",
    "y = data['class'].values\n",
    "\n",
    "# create test-train split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "np.random.seed(7)\n",
    "\n",
    "# load the dataset but only keep the top n words, zero the rest\n",
    "top_words = len(t.word_index) + 1\n",
    "\n",
    "# truncate and pad input sequences\n",
    "seq_lengths = [len(seq) for seq in X]\n",
    "max_seq_length = max(seq_lengths)\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=max_seq_length)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=max_seq_length)\n",
    "#X = sequence.pad_sequences(X, maxlen=max_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create the model using parameters from grid search \n",
    "embedding_vecor_length = 16\n",
    "drop = 0.8\n",
    "recurrent_drop = 0.8\n",
    "model = Sequential()\n",
    "model.add(Embedding(top_words, embedding_vecor_length, input_length=max_seq_length))\n",
    "model.add(Conv1D(filters=200, kernel_size=5, padding='same', activation='selu'))\n",
    "model.add(MaxPooling1D(pool_size=4))\n",
    "model.add(LSTM(150, dropout=drop, recurrent_dropout=recurrent_drop))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# record training progress\n",
    "history = model.fit(X_train, y_train, epochs=25, batch_size=64, validation_data=(X_test, y_test))\n",
    "\n",
    "# Final evaluation of the model\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# plot loss vs. epoch\n",
    "# https://machinelearningmastery.com/diagnose-overfitting-underfitting-lstm-models/\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model train vs validation loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper right')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
